{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AIzaSyB85L22CtFtq5mDSen5YsN8dzyGSBrdtsk\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "print(GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "def get_pdf_text(pdf_docs):\n",
    "    text=\"\"\n",
    "    for pdf in pdf_docs:\n",
    "        pdf_reader= PdfReader(pdf)\n",
    "        for page in pdf_reader.pages:\n",
    "            text+= page.extract_text()\n",
    "    return  text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "file_path=r\"C:\\Users\\Welcome\\Desktop\\Chatbot\\Information_Retrival_System\\Aakur_A_Perceptual_Prediction_Framework_for_Self_Supervised_Event_Segmentation_CVPR_2019_paper.pdf\"\n",
    "pdf_file = open(file_path, 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"\"\n",
    "for page in pdf_reader.pages:\n",
    "    text+= page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Perceptual Prediction Framework for Self Supervised Event Segmentation\\nSathyanarayanan N. Aakur\\nUniversity of South Florida\\nTampa, FL, USA\\nsaakur@mail.usf.eduSudeep Sarkar\\nUniversity of South Florida\\nTampa, FL, USA\\nsarkar@usf.edu\\nAbstract\\nTemporal segmentation of long videos is an important\\nproblem, that has largely been tackled through supervised\\nlearning, often requiring large amounts of annotated train-\\ning data. In this paper, we tackle the problem of self-\\nsupervised temporal segmentation that alleviates the need\\nfor any supervision in the form of labels (full supervision)\\nor temporal ordering (weak supervision). We introduce a\\nself-supervised, predictive learning framework that draws\\ninspiration from cognitive psychology to segment long, vi-\\nsually complex videos into constituent events. Learning in-\\nvolves only a single pass through the training data. We also\\nintroduce a new adaptive learning paradigm that helps re-\\nduce the effect of catastrophic forgetting in recurrent neural\\nnetworks. Extensive experiments on three publicly avail-\\nable datasets - Breakfast Actions, 50 Salads, and INRIA\\nInstructional Videos datasets show the efﬁcacy of the pro-\\nposed approach. We show that the proposed approach out-\\nperforms weakly-supervised and unsupervised baselines by\\nup to24% and achieves competitive segmentation results\\ncompared to fully supervised baselines with only a single\\npass through the training data. Finally, we show that the\\nproposed self-supervised learning paradigm learns highly\\ndiscriminating features to improve action recognition.\\n1. Introduction\\nVideo data can be seen as a continuous, dynamic stream\\nof visual cues encoded in terms of coherent, stable struc-\\ntures called “ events ”. Computer vision research has largely\\nfocused on the problem of recognizing and describing these\\nevents in terms of either labeled actions [ 19,18,19,2,1]\\nor sentences (captioning) [ 1,36,35,13,5,39]. Such ap-\\nproaches assume that the video is already segmented into\\natomic, stable units sharing a semantic structure such as\\n“throw ball ” or “ pour water ”. However, the problem of\\ntemporally localizing events in untrimmed video has not\\nbeen explored to the same extent as activity recognition or\\ncaptioning. In this work, we aim to tackle the problem oftemporal segmentation of untrimmed videos into its con-\\nstituent events in a self-supervised manner, without the need\\nfor training data annotations.\\nGT\\nObserved\\xa0\\nFeature\\xa0\\nPredicted\\xa0\\nFeature\\xa0\\nPredicted\\xa0\\nSegments\\xa0Self\\xa0Supervised\\xa0\\nError\\xa0\\nTake Bowl Crack Eggs Spoon Flour\\nFigure 1: Proposed Approach : Given an unsegmented in-\\nput video, we encode it into a higher level feature. We pre-\\ndict the feature for next time instant. A self-supervised sig-\\nnal based on the difference between the predicted and the\\nobserved feature gives rise to a possible event boundary.\\nTo segment a video into its constituent events , we must\\nﬁrst deﬁne the term event . Drawing from cognitive psychol-\\nogy [ 42], we deﬁne an event to be a “ segment of time at a\\ngiven location that is perceived by an observer to have a\\nbeginning and an end ”. Event segmentation is the process\\nof identifying these beginnings and endings and their re-\\nlations. Based on the level of distinction, the granularity of\\nthese events can be variable. For example, throw ball andhit\\nballcan be events that constitute a larger, overarching event\\nplay baseball . Hence, each event can be characterized by a\\nstable, internal representation that can be used to anticipate\\nfuture visual features within the same event with high cor-\\nrelation, with increasing levels of error as the current tran-\\nsitions into the next. Self-supervised learning paradigms of\\n“predict, observe and learn ” can then be used to provide\\n1\\n1197\\nsupervision for training a computational model, typically a\\nneural network with recurrence for temporal coherence.\\nWe propose a novel computational model1based on the\\nconcept of perceptual prediction. Deﬁned in cognitive psy-\\nchology, it refers to the hierarchical process that transforms\\nthe current sensory inputs into state representations of the\\nnear future that allow for actions. Such representation of\\nthe near future enables us to anticipate sensory informa-\\ntion based on the current event. This is illustrated in Figure\\n1. The features were visualized using T-SNE [ 23] for pre-\\nsentation. The proposed approach has three characteristics.\\nIt is hierarchical, recurrent and cyclical. The hierarchical\\nnature of the proposed approach lies in the abstraction of\\nthe incoming video frames into features of lower variabil-\\nity that is conducive to prediction. The proposed model is\\nalso recurrent. The predicted features are highly dependent\\non the current and previous states of the network. Finally,\\nthe model is highly cyclical. Predictions are compared con-\\ntinuously to observed features and are used to guide fu-\\nture predictions. These characteristics are common work-\\ning assumptions in many different theories of perception\\n[26], neuro-physiology [ 11,7], language processing [ 34]\\nand event perception[ 14].\\nContributions: The contributions of our proposed ap-\\nproach are three-fold. (1) We are, to the best of our knowl-\\nedge, the ﬁrst to tackle the problem of self-supervised, tem-\\nporal segmentation of videos. (2) We introduce the no-\\ntion of self-supervised predictive learning for active event\\nsegmentation. (3) We show that understanding the spatial-\\ntemporal dynamics of events enable the model to learn the\\nvisual structure of events for better activity recognition.\\n2. Related Work\\nFully supervised approaches treat event segmentation\\nas a supervised learning problem and assign the seman-\\ntics to the video in terms of labels and try to segment the\\nvideo into its semantically coherent “ chunks ”, with contigu-\\nous frames sharing the same label. There have been dif-\\nferent approaches to supervised action segmentation such\\nas frame-based labeling using handcrafted features and a\\nsupport vector machine [ 19], modeling temporal dynamics\\nusing Hidden Markov Models [ 19], temporal convolutional\\nneural networks (TCN) [ 27], spatiotemporal convolutional\\nneural networks (CNN) [ 21] and recurrent networks [ 29] to\\nname a few. Such approaches often rely on the quantity and\\nquality of the training annotations and constrained by the\\nsemantics captured in the training annotations, i.e., a closed\\nworld assumption.\\nWeakly supervised approaches have also been ex-\\nplored to an extent to alleviate the need for large amounts of\\n1Additional results and code can be found at\\nhttp:\\\\\\\\www.eng.usf.edu \\\\cvprglabeled data. The underlying concept behind weak supervi-\\nsion is to alleviate the need for direct labeling by leveraging\\naccompanying text scripts or instructions as indirect super-\\nvision for learning highly discriminant features. There have\\nbeen two common approaches to weakly supervised learn-\\ning for temporal segmentation of videos - (1) using script\\nor instructions for weak annotation[ 6,10,3,24], and (2)\\nfollowing an incomplete temporal localization of actions\\nfor learning and inference[ 16,29]. While such approaches\\nmodel the temporal transitions using RNNs, they still rely\\non enforcing semantics for segmenting actions and hence\\nrequire some supervision for learning and inference.\\nUnsupervised learning has not been explored to the\\nsame extent as supervised approaches, primarily because\\nlabel semantics, if available, aid in segmentation. The pri-\\nmary approach is to use clustering as the unsupervised ap-\\nproach using discriminant features[ 4,30]. The models in-\\ncorporate a temporal consistency into the segmentation ap-\\nproach by using either LSTMs [ 4] or generalized mallows\\nmodel [ 30]. Garcia et al. [12] explore the use of a genera-\\ntive LSTM network to segment sequences like we do, how-\\never, they handle only coarse temporal resolution in life-log\\nimages sampled as far apart as 30 seconds. Consecutive\\nimages when events change have more variability making\\nfor easier discrimination. Besides, they require an iterative\\ntraining process, which we do not.\\n3. Perceptual Prediction Framework\\nIn this section, we introduce the proposed framework.\\nWe begin with a discussion on the perceptual processing\\nunit, including encoding, prediction and feature reconstruc-\\ntion. We continue with an explanation of the self-supervised\\napproach for training the model, followed by a discussion\\non boundary detection and adaptive learning. We conclude\\nwith implementation details of the proposed approach. It is\\nto be noted that [ 25] also propose a similar approach based\\non the Event Segmentation Theory. However, the event\\nboundary detection is achieved using a reinforcement learn-\\ning paradigm that requires signiﬁcant amounts of training\\ndata and iterations and the approach has only been demon-\\nstrated on motion capture data.\\n3.1. Perceptual Processing\\nWe follow the general principles outlined in the Event\\nSegmentation Theory proposed by Zacks et al. [41,42,40].\\nAt the core of the approach, illustrated in Figure 2is a pre-\\ndictive processing platform that encodes a visual input I(t)\\ninto a higher level abstraction I′(t)using an encoder net-\\nwork. The abstracted feature is used as a prior to predict the\\nanticipated feature I′(t+1) at timet+1. The reconstruction\\nor decoder network creates the anticipated feature, which is\\nused to determine the event boundaries between successive\\nactivities in streaming, input video.\\n1198\\n\\xa0\\xa0\\xa0I'tEncoding\\xa0\\xa0\\nNetwork\\nInput\\xa0Frame\\xa0at\\xa0time\\xa0 t\\xa0\\xa0\\xa0htInput\\xa0Frame\\xa0at\\xa0time\\xa0 t+1\\xa0\\xa0\\xa0\\xa0\\xa0I't+1Encoding\\xa0\\xa0\\nNetwork\\n\\xa0\\xa0\\xa0y't+1\\xa0\\nDecoding\\xa0\\nNetwork\\xa0\\xa0\\xa0et+1Perceptual\\xa0Prediction\\xa0\\xa0\\nError\\xa0Detection\\xa0\\xa0\\nLearning\\xa0Signal\\xa0\\n\\xa0\\nLearning\\xa0Signal\\xa0\\n\\xa0\\nLearning\\xa0Signal\\xa0\\n\\xa0Event\\xa0Boundary\\xa0\\xa0\\nDecision\\xa0\\xa0\\nItNew\\xa0Event/\\xa0\\nSame\\xa0Event\\xa0\\xa0\\xa0It+1\\nFigure 2: Overall architecture : The proposed approach consists of four essential components: an encoder network, a\\npredicting unit, a decoding network, and error detection and boundary decision unit.\\n3.1.1 Visual Feature Encoding\\nWe encode the input frame at each time step into an ab-\\nstracted, higher level visual feature and use it as a basis\\nfor perceptual processing rather than the raw input at the\\npixel level (for reduced network complexity) or higher level\\nsemantics (which require training data in the form of la-\\nbels). The encoding process requires learning a function\\ng(I(t),ωe)that transforms an input frame I(t)into a higher\\ndimensional feature space that encodes the spatial features\\nof the input into a feature vector I′(t), whereωeis the\\nset of learnable parameters. While the feature space can\\nbe pre-computed features such as Histogram of Optic Flow\\n(HOF) [ 8], or Improved Dense Trajectories (IDT) [ 38], we\\npropose the joint training of a convolutional neural network.\\nThe prediction error and the subsequent error gradient\\ndescribed in Sections 3.3and3.4, respectively, allow for\\nthe CNN to learn highly discriminative features, resulting in\\nhigher recognition accuracy (Section 4.4.1 ). An added ad-\\nvantage is that the prediction can be made at different hier-\\narchies of feature embeddings, including at the pixel-level,\\nallowing for event segmentation at different granularities.\\n3.1.2 Recurrent Prediction for Feature Forecasting\\nThe prediction of the visual feature at time t+ 1is condi-\\ntioned by the observation at time t,I′(t), and an internal\\nmodel of the current event. Formally, this can be deﬁned by\\na generative model P(I′(t+1)|ωp,I′(t)), whereωpis theset of hidden parameters characterizing the internal state of\\nthe current observed event. To capture the temporal depen-\\ndencies among intra -event frames and inter -event frames,\\nwe propose the use of a recurrent network, such as recur-\\nrent neural networks (RNN) or Long Short Term Memory\\nNetworks (LSTMs)[ 15]. The predictor model can be math-\\nematically expressed as\\nit=σ(WiI′(t)+Whiht−1+bi) (1)\\nft=σ(WfI′(t)+Whfht−1+bf)\\not=σ(WoI′(t)+Whoht−1+bo)\\ngt=φ(WgI′(t)+Whght−1+bg)\\nmt=ft·mt−1+it·gt\\nht=ot·φ(mt)\\nwhereσis a non-linear activation function, the dot-operator\\n(·) represents element-wise multiplication, φis the hyper-\\nbolic tangent function ( tanh) andWxandbxrepresent the\\ntrained weights and biases for each of the gates. Collec-\\ntively,{Whi,Whf,Who,Whg}and their respective biases\\nconstitute the learnable parameters ωp.\\nAs can be seen from Equation 1, there are four common\\n“gates” or layers that help the prediction of the network -\\nthe input gate it, forget gate ft, output gate ot, the mem-\\nory layergt, the memory state mtand the event state ht.\\nIn the proposed framework, the memory state mtand the\\nevent statehtare key to the predictions made by the re-\\ncurrent unit. The event state htis a representation of the\\n1199\\nevent observed at time instant tand hence is sensitive to\\nthe observed input I′(t)than the event layer, which is more\\npersistent across events. The event layer is a gated layer,\\nwhich receives input from the encoder as well as the recur-\\nrent event model. However, the inputs to the event layer are\\nmodulated by a self-supervised gating signal (Section 3.3),\\nwhich is indicative of the quality of predictions made by the\\nrecurrent model. The gating allows for updating the weights\\nquickly but also maintains a coherent state within the event.\\nWhy recurrent networks? While convolutional de-\\ncoder networks [ 17] and mixture-of-network models [ 37]\\nare viable alternatives for future prediction, we propose the\\nuse of recurrent networks for the following reasons. Imag-\\nine a sequence of frames Ia= (I1\\na,I2\\na,...In\\na)correspond-\\ning to the activity a. Given the complex nature of videos\\nsuch as those in instructional or sports domains, the next\\nset of frames can be followed by frames of activity borc\\nwith equal probability, given by Ib= (I1\\nb,I2\\nb,...Im\\nb)and\\nIc= (I1\\nc,I2\\nc,...Ik\\nc)respectively. Using a fully connected\\nor convolutional prediction unit is likely to result in the pre-\\ndiction of features that tend to be the average of the two\\nactivitiesaandb, i.e.Ik\\navg=1\\n2(Ik\\nb+Ik\\nc)for the time k.\\nThis is not a desirable outcome because the predicted fea-\\ntures can either be an unlikely outcome or, more probably,\\nbe outside the plausible manifold of representations. The\\nuse of recurrent networks such as RNNs and LSTMs allow\\nfor multiple futures that can be possible at time t+1, con-\\nditioned upon the observation of frames until time t.\\n3.1.3 Feature Reconstruction\\nIn the proposed framework, the goal of the perceptual pro-\\ncessing unit (or rather the reconstruction network) is to re-\\nconstruct the predicted feature y′\\nt+1given a source predic-\\ntionht, which maximizes the probability\\np(y′\\nt+1|ht)∝p(ht|y′\\nt+1)p(y′\\nt+1) (2)\\nwhere the ﬁrst term is the likelihood model (or translation\\nmodel from NLP) and the second is the feature prior model.\\nHowever, we model logp(y′\\nt+1|ht)as a log-linear model\\nf(·)conditioned upon the weights of the recurrent model\\nωpand the observed feature I′(t)and characterized by\\nlogp(y′\\nt+1|ht) =t/summationdisplay\\nn=1f(ωp,I′(t))+logZ(ht) (3)\\nwhereZ(ht)is a normalization constant that does not de-\\npend on the weights ωp. The reconstruction model com-\\npletes the generative process for forecasting the feature at\\ntimet+ 1 and helps in constructing the self-supervised\\nlearning setting for identifying event boundaries.3.2. Self\\xadSupervised Learning\\nThe quality of the predictions is determined by compar-\\ning the prediction from the predictor model y′(t)to the ob-\\nserved visual feature I′(t). The deviation of the predicted\\ninput from the observed features is termed as the perceptual\\nprediction error EP(t)and is described by the equation:\\nEP(t) =n/summationdisplay\\ni=1∝bardblI′(t)−y′(t)∝bardbl2\\nℓ1(4)\\nwhereEP(t)is the perceptual prediction error at time t,\\ngiven the predicted visual y′(t)and the actual observed fea-\\nture at timet,I′(t). The predicted input is obtained through\\nthe inference function deﬁned in Equation 2. The percep-\\ntual prediction error is indicative of the prediction quality\\nand is directly correlated with the quality of the recurrent\\nmodel’s internal state h(t). Increasingly large deviations\\nindicate that the current state is not a reliable representation\\nof the observed event. Hence, the gating signal serves as\\nan indicator of event boundaries. The minimization of the\\nperceptual prediction error serves as the objective function\\nfor the network during training.\\n3.3. Error Gating for Event Segmentation\\nThe gating signal (Section 3) is an integral component in\\nthe proposed framework. We hypothesize that the visual\\nfeatures of successive events differ signiﬁcantly at event\\nboundaries. The difference in visual features can be mi-\\nnor among sub-activities and can be large across radically\\ndifferent events. For example, in Figure 1, we can see that\\nthe visual representation of the features learned by the en-\\ncoder network for the activities take bowl andcrack eggs are\\ncloser together than the features between the activities take\\nbowl andspoon ﬂour . This diverging feature space causes\\na transient increase in the perceptual prediction error, espe-\\ncially at event boundaries. The prediction error decreases as\\nthe predictor model adapts to the new event. This is illus-\\ntrated in Figure 1. We show the perceptual prediction error\\n(second from the bottom) and the ground truth segmenta-\\ntion (second from the top) for the video Make Pancake. As\\nillustrated, the error rates are higher at the event boundaries\\nand lower among “in-event” frames.\\nThe unsupervised gating signal is achieved using an\\nanomaly detection module. In our implementation, we use\\na low pass ﬁlter. The low pass ﬁlter maintains a relative\\nmeasure of the perceptual prediction error made by the pre-\\ndictor module. It is a relative measure because the low pass\\nﬁlter only maintains a running average of the prediction er-\\nrors made over the last n time steps. The perceptual quality\\nmetric,Pq, is given by:\\nPq(t) =Pq(t−1)+1\\nn(EP(t)−Pq(t−1)) (5)\\n1200\\nwherenis the prediction error history that inﬂuences the\\nanomaly detection module’s internal model for detecting\\nevent boundaries. In our experiments, we maintain nat 5.\\nThis is chosen based on the average response time of human\\nperception, which is around 200 ms [ 33].\\nThe gating signal, G(t), is triggered when the current\\nprediction error exceeds the average quality metric by at\\nleast50%.\\nG(t) =/braceleftBigg\\n1,EP(t)\\nPq(t−1)>ψe\\n0,otherwise(6)\\nwherePE(t)is the perceptual prediction error at time t,\\nG(t)is the value of the gating signal at time t,Pq(t−1)\\nis the prediction quality metric at time tandΨeis the pre-\\ndiction error threshold for boundary detection. For opti-\\nmal prediction, the perceptual prediction error would be\\nvery high at the event boundary frames and very low at all\\nwithin-event frames. In our experiments, Ψeis set to be 1.5.\\nIn actual, real-world video frames, however, there exist\\nadditional noise in the form of occlusions and background\\nmotion which can cause some event boundaries to have a\\nlow perceptual prediction error. In that case, however, the\\ngating signal would continue to be low and become high\\nwhen there is a transient increase in error. This is visualized\\nin Figure 1. It can be seen that the perceptual errors were\\nlower at event boundaries between activities take bowl and\\ncrack eggs in a video of ground truth make pancakes . How-\\never, the prediction error increases radically soon after the\\nboundary frames, indicating a new event. Such cases could,\\narguably, be attributed to conditions when there are lesser\\nvariations in the visual features at an event boundary.\\n3.4. Adaptive Learning for Plasticity\\nThe proposed training of the prediction module is partic-\\nularly conducive towards overﬁtting since we propagate the\\nperceptual prediction error at every time step. This intro-\\nduces severe overﬁtting, especially in the prediction model.\\nTo allow for some plasticity and avoid catastrophic for-\\ngetting in the network, we introduce the concept of adap-\\ntive learning. This is similar to the learning rate sched-\\nule, a commonly used technique for training deep neural\\nnetworks. However, instead of using predetermined inter-\\nvals for changing the learning rates, we propose the use of\\nthe gating signal to modulate the learning rate. For exam-\\nple, when the perceptual prediction rate is lower than the\\naverage prediction rate, the predictor model is considered\\nto have a good, stable representation of the current event.\\nPropagating the prediction error, when there is a good rep-\\nresentation of the event can lead to overﬁtting of the pre-\\ndictor model to that particular event and does not help gen-\\neralize. Hence, we propose lower learning rates for time\\nsteps when there are negligible prediction error and a rel-\\natively higher (by a magnitude of 100) for when there ishigher prediction error. Intuitively, this adaptive learning\\nrate allows the model to adapt much quicker to new events\\n(at event boundaries where there are likely to be higher er-\\nrors) and learn to maintain the internal representation for\\nwithin-event frames.\\nFormally, the learning rate is deﬁned as the result of the\\nadaptive learning rule deﬁned as a function of the perceptual\\nprediction error deﬁned in Section 3.2and is deﬁned as\\nλlearn=\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3∆−\\ntλinit, EP(t)>µe\\n∆+\\ntλinit, EP(t)<µe\\nλinit, otherwise(7)\\nwhere∆−\\nt,∆+\\ntandλinitrefer to the scaling of the learning\\nrate in the negative direction, positive direction and the ini-\\ntial learning rate respectively and µe=1\\nt2−t1/integraltextt2\\nt1EPdEP.\\nThe learning rate is adjusted based on the quality of the\\npredictions characterized by the perceptual prediction er-\\nror between a temporal sequence between times t1andt2,\\ntypically deﬁned by the gating signal.. The impact of the\\nadaptive changes to the learning rate is shown in the quan-\\ntitative evaluation Section 4.4, where the adaptive learn-\\ning scheme shows improvement of up to 20% compared to\\ntraining without the learning scheme.\\n3.5. Implementation Details\\nIn our experiments, we use a VGG-16 [ 31] network pre-\\ntrained on ImageNet as our hierarchical, feature encoder\\nmodule. We discard the ﬁnal layer and use the second fully\\nconnected layer with 4096 units as our encoded feature vec-\\ntor for a given frame. The feature vector is then consumed\\nby a predictor model. We trained two versions, one with\\nan RNN and the other with an LSTM as our predictor mod-\\nels. The LSTM model used is the original version proposed\\nby [15]. Finally, the anomaly detection module runs an av-\\nerage low pass ﬁlter described in Section 3.3. The initial\\nlearning rate described in Section 3.4is set to be 1×10−6.\\nThe scaling factors ∆−\\ntand∆+\\ntare set to be 1×10−2and\\n1×10−3, respectively. The training was done on a computer\\nwith one Titan X Pascal.\\n4. Experimental Evaluation\\n4.1. Datasets\\nWe evaluate and analyze the performance of the pro-\\nposed approach on three large, publicly available datasets\\n- Breakfast Actions [ 19], INRIA Instructional Videos\\ndataset[ 3] and the 50 Salads dataset [ 32]. Each dataset of-\\nfers a different challenge to the approach allow us to evalu-\\nate its performance on a variety of challenging conditions.\\nBreakfast Actions Dataset is a large collection of 1,712\\nvideos of 10 breakfast activities performed by 52 actors.\\n1201\\nBG T ake\\xa0Bowl Pour\\xa0Cereals Pour\\xa0Milk Stir\\xa0Cereals BG\\nGround\\xa0truth\\nHTK\\nOCDC\\nOurs\\xa0(LSTM\\xa0+\\xa0AL)ECTC\\nFigure 3: Illustration of the segmentation performance of the proposed approach on the Breakfast Actions Dataset on a video\\nwith ground truth Make Cereals . The proposed approach does not show the tendency to over-segment and provides coherent\\nsegmentation. The approach, however, shows a tendency to take longer to detect boundaries for visually similar activities.\\nEach activity consists of multiple sub-activities that pos-\\nsess visual and temporal variations according to the sub-\\nject’s preferences and style. Varying qualities of visual data\\nas well as complexities such as occlusions and viewpoints\\nincrease the complexity of the temporal segmentation task.\\nINRIA Instructional Videos Dataset contains 150\\nvideos of 5 different activities collected from YouTube.\\nEach of the videos are, on average, 2 minutes long and have\\naround 47 sub-activities. There also exists a “background\\nactivities” which consists of sequence where there does not\\nexist a clear sub-activity that is visually discriminable. This\\noffers a considerable challenge for approaches that are not\\nexplicitly trained for such visual features.\\n50 Salads Dataset is a multimodal data collected in the\\ncooking domain. The datasets contains over four (4) hours\\nof annotated data of 25 people preparing 2 mixed salads\\neach and provides data in different modalities such as RGB\\nframes, depth maps and accelerometer data for devices at-\\ntached to different items such as knives, spoons and bottles\\nto name a few. The annotations of activities are provided\\nat different levels of granularities - high, low and eval. We\\nuse the “eval” granularity following evaluation protocols in\\nprior works [ 21,27].\\n4.2. Evaluation Metrics\\nWe use two commonly used evaluation metrics for an-\\nalyzing the performance of the proposed model. We used\\nthe same evaluation protocol and code as in [ 3,30]. We\\nused the Hungarian matching algorithm to obtain the one-\\nto-one mappings between the predicted segments and the\\nground truth to evaluate the performance due to the un-\\nsupervised nature of the proposed approach. We use the\\nmean over frames (MoF) to evaluate the ability of the pro-\\nposed approach to temporally localize the sub-activities.\\nWe evaluate the divergence of the predicted segments from\\nthe ground truth segmentation using the Jaccard index (In-\\ntersection over Union or IoU). We also use the F1 score toevaluate the quality of the temporal segmentation. The eval-\\nuation protocol for the recognition task in Section 4.4.1 is\\nthe unit level accuracy for the 48 classes as seen in Table 3\\nfrom [ 19] and compared in [ 19,1,9,16].\\n4.3. Ablative Studies\\nWe evaluate different variations of our proposed ap-\\nproach to compare the effectiveness of each proposed com-\\nponent. We varied the prediction history nand the predic-\\ntion error threshold Ψ. Increasing frame window tends to\\nmerge frames and smaller clusters near the event bound-\\naries to the prior activity class due to transient increase in\\nerror. This results in higher IoU and lower MoF. Low error\\nthreshold results in over segmentation as boundary detec-\\ntion becomes sensitive to small changes. The number of\\npredicted clusters decreases as the window size and thresh-\\nold increases. We also trained four (4) models, with dif-\\nferent predictor units. We trained two recurrent neural net-\\nworks (RNN) as the predictor units with and without adap-\\ntive learning described in Section 3.4indicated as RNN +\\nNo AL andRNN + AL , respectively. We also trained LSTM\\nwithout adaptive learning ( LSTM + No AL ) to compare\\nagainst our main model ( LSTM + AL ). We use RNNs as a\\npossible alternative due to the short-term future predictions\\n(1 frame ahead) required. We discuss these results next.\\n4.4. Quantitative Evaluation\\nBreakfast Actions Dataset We evaluate the perfor-\\nmance of our full model LSTM + AL on the breakfast ac-\\ntions dataset and compare against fully supervised, weakly\\nsupervised and unsupervised approaches. We show the per-\\nformance of the SVM[ 19] approach to highlight the impor-\\ntance of temporal modeling. As can be seen from Table 1,\\nthe proposed approach outperformed all unsupervised and\\nweakly supervised approaches, and some fully supervised\\napproaches.\\nIt should be noted that the other unsupervised ap-\\n1202\\nGround\\xa0truthLSTM\\xa0+\\xa0ALLSTM\\xa0+\\xa0No\\xa0ALRNN\\xa0+\\xa0ALRNN\\xa0+\\xa0No\\xa0AL\\nBrake\\xa0On Jack\\xa0Up\\n Put\\xa0Things\\xa0Back\\n Screw\\xa0Wheel\\nFigure 4: Ablative Studies : Illustrative comparison of variations of our approach, using RNNs and LSTMs with and without\\nadaptive learning on the INRIA Instructional Videos Dataset on a video with ground truth Change Tire . It can be seen\\nthat complex visual scenes with activities of shorter duration pose a signiﬁcant challenge to the proposed framework and\\ncause fragmentation and over segmentation. However, the use of adaptive learning helps alleviate this to some extent. Note:\\nTemporal segmentation time lines are shown without the background class for better visualization.\\nSupervision Approach MoF IoU\\nFullSVM [ 19] 15.8 -\\nHTK(64)[ 20] 56.3 -\\nED-TCN[ 27] 43.3 42.0\\nTCFPN[ 10] 52.0 54.9\\nGRU[ 29] 60.6 -\\nWeakOCDC[ 6] 8.9 23.4\\nECTC[ 16] 27.7 -\\nFine2Coarse[ 28] 33.3 47.3\\nTCFPN + ISBA[ 10]38.4 40.6\\nNoneKNN+GMM[ 30] 34.6 47.1\\nOurs (LSTM + AL) 42.9 46.9\\nTable 1: Segmentation Results on the Breakfast Action\\ndataset. MoF refers to the Mean over Frames metric and\\nIoU is the Intersection over Union metric.\\nproach [ 30], requires the number of clusters (from ground\\ntruth) to achieve the performance whereas our approach\\ndoes not require such knowledge and is done in a stream-\\ning fashion. Additionally, the weakly supervised meth-\\nods [ 16,28,10] require both the number of actions as well\\nas an ordered list of sub-activities as input. ECTC [ 16] is\\nbased on discriminative clustering, while OCDC [ 6] and\\nFine2Coarse [ 28] are also RNN-based methods.\\n50 Salads Dataset We also evaluate our approach on the\\n50 Salads dataset, using only the visual features as input.\\nWe report the Mean of Frames (MoF) metric for fair com-\\nparison. As can be seen from Table 2, the proposed ap-\\nproach signiﬁcantly outperforms the other unsupervised ap-\\nproach, improving by over 11%. We also show the perfor-mance of the frame-based classiﬁcation approaches VGG\\nand IDT [ 21] to show the impact of temporal modeling.\\nIt should be noted that the fully supervised approaches re-\\nSupervision Approach MoF\\nFullVGG**[ 21] 7.6%\\nIDT**[ 21] 54.3%\\nS-CNN + LSTM[ 21]66.6%\\nTDRN[ 22] 68.1%\\nST-CNN + Seg[ 21] 72.0%\\nTCN[ 27] 73.4%\\nNoneLSTM + KNN[ 4] 54.0%\\nOurs (LSTM + AL) 60.6%\\nTable 2: Segmentation Results on the 50 Salads dataset, at\\ngranularity ‘ Eval‘. **Models were intentionally reported\\nwithout temporal constraints for ablative studies.\\nquired signiﬁcantly more training data - both in the form of\\nlabels as well as training epochs. Additionally, the TCN ap-\\nproach [ 27] uses the accelerometer data as well to achieve\\nthe state-of-the-art performance of 74.4%\\nINRIA Instructional Videos Dataset: Finally, we\\nevaluate our approach on the INRIA Instructional Videos\\ndataset, which posed a signiﬁcant challenge in the form of\\nhigh amounts of background (noise) data. We report the\\nF1 score for fair comparison to the other state-of-the-art\\napproaches. As can be seen from Table 3, the proposed\\nmodel outperforms the other unsupervised approach [ 30]\\nby23.3%, the weakly supervised approach [ 6] by24.8%\\nand has competitive performance to the fully supervised\\napproaches[ 24,3,30].\\n1203\\nSupervision Approach F1\\nFullHMM + Text [ 24] 22.9%\\nDiscriminative Clustering[ 3]41.4%\\nKNN+GMM[ 30] + GT 69.2%\\nWeakOCDC + Text Features [ 6] 28.9%\\nOCDC [ 6] 31.8%\\nNoneKNN+GMM[ 30] 32.2%\\nOurs (RNN + No AL) 25.9%\\nOurs (RNN + AL) 29.4%\\nOurs (LSTM + No AL) 36.4%\\nOurs (LSTM + AL) 39.7%\\nTable 3: Segmentation Results on the INRIA Instructional\\nVideos dataset. We report F1 score for fair comparison.\\nWe also evaluate the performance of the models with\\nand without adaptive learning. It can be seen that long\\nterm temporal dependence captured by LSTMs is signiﬁ-\\ncant, especially due to the long durations of activities in\\nthe dataset. Additionally, the use of adaptive learning has\\na signiﬁcant improvement in the segmentation framework,\\nimproving the performance by 9%and11% for the RNN-\\nbased model and the LSTM-based model respectively, indi-\\ncating a reduced overﬁtting of the model to the visual data.\\n4.4.1 Improved Features for Action Recognition\\nTo evaluate the ability of the network to learn highly dis-\\ncriminative features for recognition, we evaluated the per-\\nformance of the proposed approach in a recognition task.\\nWe use the model pretrained on the segmentation task on\\nthe Breakfast Actions dataset and use the hidden layer of\\nthe LSTM as input to a fully connected layer and use cross\\nentropy to train the model. We also trained another net-\\nwork with the same structure - VGG16 + LSTM without\\nthe pretraining on the segmentation task to compare the ef-\\nfect of the features learned using self-supervision. As can\\nApproach Precision\\nHCF + HMM [ 19] 14.90%\\nHCF + CFG + HMM [ 19] 31.8%\\nRNN + ECTC [ 16] 35.6%\\nRNN + ECTC (Cosine) [ 16] 36.7%\\nHCF + Pattern Theory [ 9] 38.6%\\nHCF + Pattern Theory + ConceptNet[ 1] 42.9%\\nVGG16 + LSTM 33.54%\\nVGG16 + LSTM + Predictive Features(AL) 37.87%\\nTable 4: Activity recognition results on Breakfast Actions\\ndataset. HCF and AL refer to handcrafted features and\\nAdaptive Learning, respectively.\\nbe seen from Table 4, the use of self-supervision to pre-\\ntrain the network prior to the recognition task improves therecognition performance of the network and has compara-\\nble performance to the other state-of-the-art approaches. It\\nimproves the recognition accuracy by 13.12% over the net-\\nwork without predictive pretraining.\\n4.5. Qualitative Evaluation\\nThrough the predictive, self supervised framework, we\\nare able to learn the sequence of visual features in stream-\\ning video. We visualize the segmentation performance of\\nthe proposed framework on the Breakfast Actions Dataset\\nin Figure 3. It can be seen that the proposed approach has\\nhigh temporal coherence and does not suffer from over seg-\\nmentation, especially when the segments are long. Long ac-\\ntivity sequences allow the model to learn from observation\\nby providing more samples of “ intra- event” samples. Ad-\\nditionally, it can be seen that weakly supervised approaches\\nlike OCDC[ 6] and ECTC[ 16] suffer from over segmenta-\\ntion and intra-class fragmentation. This could arguably be\\nattributed to the fact that they tend to enforce semantics,\\nin the form of weak ordering of activities in the video re-\\ngardless of the changes in visual features. Fully supervised\\napproaches, such as HTK[ 20] perform better, especially due\\nto the ability to assign semantics to visual features. How-\\never, they are also affected by unbalanced data and dataset\\nshift, as can be seen in Figure 3where the background class\\nwas segmented into other classes.\\nWe also qualitatively evaluated the impact of adaptive\\nlearning and long term temporal memory in Figure 4, where\\nthe performance of the alternative methods described in\\nSection 4.4. It can be seen that the use of adaptive learn-\\ning during training allows the model to not overﬁt to any\\nsingle class’ intra-event frames and help generalize to other\\nclasses regardless of amount of training data. It is not to\\nsay that the problem of unbalanced data is alleviated, but\\nthe adaptive learning does help to some extent. It is inter-\\nesting to note that the LSTM model tends to over-segment\\nwhen not trained with adaptive learning, while the RNN-\\nbased model does not suffer from the same fate.\\n5. Conclusion\\nWe demonstrate how a self-supervised learning\\nparadigm can be used to segment long, highly complex\\nvisual sequences. There are key differences between\\nour approach and fully supervised or weakly supervised\\napproaches, including classical ones such as DBMs and\\nHMMs. At a high level, our approach is unsupervised and\\ndoes not require labeled data for training. The predictive\\nerror serves as supervision for training the framework. The\\nother major aspect is that our approach requires only a\\nsingle pass through the training data. Hence, the training\\ntime is very low. The experimental results demonstrate the\\nrobustness, high performance, and the generality of the\\napproach on multiple real world datasets.\\n1204\\nReferences\\n[1] Sathyanarayanan Aakur, Fillipe DM de Souza, and Sudeep\\nSarkar. Going deeper with semantics: Exploiting seman-\\ntic contextualization for interpretation of human activity in\\nvideos. In IEEE Winter Conference on Applications of Com-\\nputer Vision (WACV) . IEEE, 2019. 1,6,8\\n[2] Sathyanarayanan N. Aakur, Fillipe DM de Souza, and\\nSudeep Sarkar. Towards a knowledge-based approach for\\ngenerating video descriptions. In Conference on Computer\\nand Robot Vision (CRV) . Springer, 2017. 1\\n[3] Jean-Baptiste Alayrac, Piotr Bojanowski, Nishant Agrawal,\\nJosef Sivic, Ivan Laptev, and Simon Lacoste-Julien. Unsu-\\npervised learning from narrated instruction videos. In IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 4575–4583, 2016. 2,5,6,7,8\\n[4] Bharat Lal Bhatnagar, Suriya Singh, Chetan Arora, CV\\nJawahar, and KCIS CVIT. Unsupervised learning of deep\\nfeature representation for clustering egocentric actions. In\\nInternational Joint Conference on Artiﬁcial Intelligence (IJ-\\nCAI) , pages 1447–1453. AAAI Press, 2017. 2,7\\n[5] Yi Bin, Yang Yang, Fumin Shen, Xing Xu, and Heng Tao\\nShen. Bidirectional long-short term memory for video de-\\nscription. In ACM Conference on Multimedia (ACM MM) ,\\npages 436–440. ACM, 2016. 1\\n[6] Piotr Bojanowski, R ´emi Lajugie, Francis Bach, Ivan Laptev,\\nJean Ponce, Cordelia Schmid, and Josef Sivic. Weakly su-\\npervised action labeling in videos under ordering constraints.\\nInEuropean Conference on Computer Vision (ECCV) , pages\\n628–643. Springer, 2014. 2,7,8\\n[7] Gail A Carpenter and Stephen Grossberg. Adaptive reso-\\nnance theory . Springer, 2016. 2\\n[8] Rizwan Chaudhry, Avinash Ravichandran, Gregory Hager,\\nand Ren ´e Vidal. Histograms of oriented optical ﬂow and\\nbinet-cauchy kernels on nonlinear dynamical systems for the\\nrecognition of human actions. In IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 1932–\\n1939. IEEE, 2009. 3\\n[9] Fillipe DM de Souza, Sudeep Sarkar, Anuj Srivastava, and\\nJingyong Su. Spatially coherent interpretations of videos us-\\ning pattern theory. International Journal on Computer Vision\\n(IJCV) , pages 1–21, 2016. 6,8\\n[10] Li Ding and Chenliang Xu. Weakly-supervised action seg-\\nmentation with iterative soft boundary assignment. In IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , 2018. 2,7\\n[11] Joaquin M Fuster. The prefrontal cortex and its relation to\\nbehavior. In Progress in brain research , volume 87, pages\\n201–211. Elsevier, 1991. 2\\n[12] Ana Garcia del Molino, Joo-Hwee Lim, and Ah-Hwee Tan.\\nPredicting visual context for unsupervised event segmenta-\\ntion in continuous photo-streams. In ACM Conference on\\nMultimedia (ACM MM) , pages 10–17. ACM, 2018. 2\\n[13] Zhao Guo, Lianli Gao, Jingkuan Song, Xing Xu, Jie Shao,\\nand Heng Tao Shen. Attention-based lstm with semantic\\nconsistency for videos captioning. In ACM Conference on\\nMultimedia (ACM MM) , pages 357–361. ACM, 2016. 1[14] Catherine Hanson and Stephen Jos ´e Hanson. Development\\nof schemata during event parsing: Neisser’s perceptual cycle\\nas a recurrent connectionist network. Journal of Cognitive\\nNeuroscience , 8(2):119–134, 1996. 2\\n[15] Sepp Hochreiter and J ¨urgen Schmidhuber. Long short-term\\nmemory. Neural computation , 9(8):1735–1780, 1997. 3,5\\n[16] De-An Huang, Li Fei-Fei, and Juan Carlos Niebles. Con-\\nnectionist temporal modeling for weakly supervised action\\nlabeling. In European Conference on Computer Vision\\n(ECCV) , pages 137–153. Springer, 2016. 2,6,7,8\\n[17] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V\\nGool. Dynamic ﬁlter networks. In Neural Information Pro-\\ncessing Systems , pages 667–675, 2016. 4\\n[18] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas\\nLeung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video\\nclassiﬁcation with convolutional neural networks. In IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 1725–1732, 2014. 1\\n[19] Hilde Kuehne, Ali Arslan, and Thomas Serre. The language\\nof actions: Recovering the syntax and semantics of goal-\\ndirected human activities. In IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR) , pages 780–\\n787, 2014. 1,2,5,6,7,8\\n[20] Hilde Kuehne, Juergen Gall, and Thomas Serre. An end-to-\\nend generative framework for video segmentation and recog-\\nnition. In IEEE Winter Conference on Applications of Com-\\nputer Vision (WACV) , pages 1–8. IEEE, 2016. 7,8\\n[21] Colin Lea, Austin Reiter, Ren ´e Vidal, and Gregory D Hager.\\nSegmental spatiotemporal cnns for ﬁne-grained action seg-\\nmentation. In European Conference on Computer Vision\\n(ECCV) , pages 36–52. Springer, 2016. 2,6,7\\n[22] Peng Lei and Sinisa Todorovic. Temporal deformable resid-\\nual networks for action segmentation in videos. In IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , pages 6742–6751, 2018. 7\\n[23] Laurens van der Maaten and Geoffrey Hinton. Visualizing\\ndata using t-sne. Journal of Machine Learning Research ,\\n9(Nov):2579–2605, 2008. 2\\n[24] Jonathan Malmaud, Jonathan Huang, Vivek Rathod, Nick\\nJohnston, Andrew Rabinovich, and Kevin Murphy. What’s\\ncookin’? interpreting cooking videos using text, speech and\\nvision. arXiv preprint arXiv:1503.01558 , 2015. 2,7,8\\n[25] Katherine Metcalf and David Leake. Modelling unsuper-\\nvised event segmentation: Learning event boundaries from\\nprediction errors. In CogSci , 2017. 2\\n[26] Ulric Neisser. Cognitive psychology new york: Appleton-\\ncentury-crofts. Google Scholar , 1967. 2\\n[27] Colin Lea Michael D Flynn Ren ´e and Vidal Austin Reiter\\nGregory D Hager. Temporal convolutional networks for ac-\\ntion segmentation and detection. In IEEE International Con-\\nference on Computer Vision (ICCV) , 2017. 2,6,7\\n[28] Alexander Richard and Juergen Gall. Temporal action detec-\\ntion using a statistical language model. In IEEE Conference\\non Computer Vision and Pattern Recognition (CVPR) , pages\\n3131–3140, 2016. 7\\n[29] Alexander Richard, Hilde Kuehne, and Juergen Gall. Weakly\\nsupervised action learning with rnn based ﬁne-to-coarse\\n1205\\nmodeling. In IEEE Conference on Computer Vision and Pat-\\ntern Recognition (CVPR) , volume 1, page 3, 2017. 2,7\\n[30] Fadime Sener and Angela Yao. Unsupervised learning and\\nsegmentation of complex activities from video. In IEEE\\nConference on Computer Vision and Pattern Recognition\\n(CVPR) , 2018. 2,6,7,8\\n[31] Karen Simonyan and Andrew Zisserman. Very deep convo-\\nlutional networks for large-scale image recognition. arXiv\\npreprint arXiv:1409.1556 , 2014. 5\\n[32] Sebastian Stein and Stephen J McKenna. Combining em-\\nbedded accelerometers with computer vision for recogniz-\\ning food preparation activities. In ACM International Joint\\nConference on Pervasive and Ubiquitous Computing , pages\\n729–738. ACM, 2013. 5\\n[33] Simon Thorpe, Denis Fize, and Catherine Marlot. Speed\\nof processing in the human visual system. Nature ,\\n381(6582):520, 1996. 5\\n[34] Teun Adrianus Van Dijk, Walter Kintsch, and Teun Adrianus\\nVan Dijk. Strategies of discourse comprehension . Academic\\nPress New York, 1933. 2\\n[35] Subhashini Venugopalan, Marcus Rohrbach, Jeffrey Don-\\nahue, Raymond Mooney, Trevor Darrell, and Kate Saenko.\\nSequence to sequence-video to text. In IEEE International\\nConference on Computer Vision (ICCV) , pages 4534–4542,\\n2015. 1\\n[36] Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Mar-\\ncus Rohrbach, Raymond Mooney, and Kate Saenko. Trans-\\nlating videos to natural language using deep recurrent neural\\nnetworks. arXiv preprint arXiv:1412.4729 , 2014. 1\\n[37] Carl V ondrick, Hamed Pirsiavash, and Antonio Torralba. An-\\nticipating visual representations from unlabeled video. In\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion (CVPR) , pages 98–106, 2016. 4\\n[38] Heng Wang and Cordelia Schmid. Action recognition with\\nimproved trajectories. In IEEE International Conference on\\nComputer Vision (ICCV) , pages 3551–3558, 2013. 3\\n[39] Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas,\\nChristopher Pal, Hugo Larochelle, and Aaron Courville. De-\\nscribing videos by exploiting temporal structure. In IEEE\\nInternational Conference on Computer Vision (ICCV) , pages\\n4507–4515, 2015. 1\\n[40] Jeffrey M Zacks and Khena M Swallow. Event segmentation.\\nCurrent Directions in Psychological Science , 16(2):80–84,\\n2007. 2\\n[41] Jeffrey M Zacks and Barbara Tversky. Event structure in\\nperception and conception. Psychological bulletin , 127(1):3,\\n2001. 2\\n[42] Jeffrey M Zacks, Barbara Tversky, and Gowri Iyer. Perceiv-\\ning, remembering, and communicating structure in events.\\nJournal of Experimental Psychology: General , 130(1):29,\\n2001. 1,2\\n1206\\n\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
